{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Example\n",
    "\n",
    "#### Install the below dependencies using pip install or you can add it to your conda environment as well.\n",
    "\n",
    "1. logging \n",
    "2. requests \n",
    "3. bs4\n",
    "4. os\n",
    "5. pandas\n",
    "6. pyodbc\n",
    "\n",
    "#### The code will start adding all csv files from BTS.gov to pipeline_etl folder and the add all data into 'airlines' database in 'airline_stats' table.\n",
    "\n",
    "###### Note : The code will extract all the data from BTS.gov ( around 20GB ) using Beautiful soup and hence will take some time to complete since for each airport and airline combination 2 requests are required to be sent.\n",
    "\n",
    "\n",
    "### Queries for Database \n",
    "\n",
    "##### Connection String used : 'Driver={ODBC Driver 17 for SQL Server};''Server=localhost;''Database=airlines;''Trusted_Connection=yes;' .\n",
    "\n",
    "##### By default we are using localhost and windows authentication. You can change it in loading.py in get_connection function.\n",
    "\n",
    "1. CREATE DATABASE airlines\n",
    "2. CREATE TABLE airline_stats ( source VARCHAR(255), destination VARCHAR(255),\n",
    "    carrier VARCHAR(255), date DATE, flightNumber VARCHAR(255), \n",
    "    scheduledDepartureTime VARCHAR(255), actualDepartureTime VARCHAR(255), \n",
    "    scheduledElapsedMinutes VARCHAR(255), actualElapsedMinutes VARCHAR(255), \n",
    "    departureDelayMinutes VARCHAR(255), wheelsOffTime VARCHAR(255), taxiOutMinutes VARCHAR(255), \n",
    "    delayCarrierMinutes VARCHAR(255), delayWeatherMinutes VARCHAR(255), \n",
    "    delayNationalAviationSystemMinutes VARCHAR(255), delaySecurityMinutes VARCHAR(255), \n",
    "    delayLateAircraftArrivalMinutes VARCHAR(255)) -- This is optional since we are creating it using python\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#pylint: disable=wrong-import-position\n",
    "sys.path.append(\"pipeline\")\n",
    "import Process.processing\n",
    "import Extract.extract\n",
    "import Load.loading\n",
    "#pylint: disable=wrong-import-position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/pipeline_etl\"\n",
    "#Extracts all the data from BTS.gov\n",
    "Extract.extract.extract_main(path) # Optionally replace with update_main(path) to update the existing data\n",
    "#Processes data collected by extraction and makes it ready to store\n",
    "Process.processing.processing_main(path)\n",
    "#Loads data from process into SQL Server\n",
    "Load.loading.loading_main(path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
