{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Example\n",
    "\n",
    "#### Install the below dependencies using pip install or you can add it to your conda environment as well.\n",
    "\n",
    "1. logging \n",
    "2. requests \n",
    "3. bs4\n",
    "4. os\n",
    "5. pandas\n",
    "6. pyodbc\n",
    "\n",
    "#### The code will start adding all csv files from BTS.gov to pipeline_etl folder and the add all data into 'airlines' database in 'airline_stats' table.\n",
    "\n",
    "###### Note : The code will extract all the data from BTS.gov ( around 20GB ) using Beautiful soup and hence will take some time to complete since for each airport and airline combination 2 requests are required to be sent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#pylint: disable=wrong-import-position\n",
    "sys.path.append(\"pipeline\")\n",
    "import Process.processing\n",
    "import Extract.extract\n",
    "import Load.loading\n",
    "#pylint: disable=wrong-import-position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/pipeline_etl\"\n",
    "#Extracts all the data from BTS.gov\n",
    "Extract.extract.extract_main(path)\n",
    "#Processes data collected by extraction and makes it ready to store\n",
    "Process.processing.processing_main(path)\n",
    "#Loads data from process into SQL Server\n",
    "Load.loading.loading_main(path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
